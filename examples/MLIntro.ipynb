{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's going on?\n",
    "\n",
    "This is a *Jupyter notebook*. It is like a lab notebook that you can write and execute python code in. It lets you interactively break down the steps of computation and see intermediate results and visualizations. \n",
    "\n",
    "\n",
    "You can write python (or markdown) in these *cells*, and press `shift` + `enter` to run the code. Anything that you print will appear below the cell.  \n",
    "\n",
    "Try printing \"Hello HaC\" in the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write Python in these cells!\n",
    "print(\"Hello HaC!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you execute a cell, it will say `In [ ]:` next to it:\n",
    "\n",
    "<img src=\"img/before.png\"></img>\n",
    "<br>\n",
    "\n",
    "When a cell is executing, it will say `In [*]:` next to it to indicate that it is still running:\n",
    "<img src=\"img/load.png\"></img>\n",
    "\n",
    "<br>\n",
    "When a cell has finished executing, it will say `In [n]:` to indicate that the cell has completed running, and that it was the `nth` cell to be evaluated:\n",
    "<img src=\"img/after.png\"></img>\n",
    "\n",
    "That should be all you need to know to do this workshop -- but Jupyter has a lot of features -- check out <a href=\"http://jupyter.readthedocs.io/en/latest/\"> the documentation for more information. </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Let's import some packages\n",
    "%pylab inline\n",
    "figsize(8,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "    <img src=\"img/scikit-learn-logo.png\" width=\"40%\" />\n",
    "    <br />\n",
    "    <h1>An introduction to Machine Learning with Scikit-Learn</h1>\n",
    "    <br /><br />\n",
    "    Parts of this notebook have been adapted from <a href=\"https://github.com/glouppe/tutorial-sklearn-lhcb/blob/master/An%20introduction%20to%20Machine%20Learning%20with%20Scikit-Learn.ipynb\">this notebook created by Gilles Louppe,</a> core developer of Scikit-Learn.\n",
    "    <br /><br />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Prerequisites "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Python 3 distribution with scientific packages (NumPy, SciPy, Scikit-Learn, Pandas)\n",
    "\n",
    "- ... or Anaconda http://continuum.io/downloads\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Outline\n",
    "\n",
    "* What is machine learning?\n",
    "* Scikit-Learn\n",
    "* Classifying hand written digits with KNN Classifier\n",
    "* Model evaluation and selection\n",
    "* Going further\n",
    "* Summary\n",
    "\n",
    "\n",
    "# Disclaimer \n",
    "\n",
    "This workshop will introduce you to some common software tools for Machine Learning, and explore a workflow of applying ML to data. It is by no means comprehensive and does not rigorously cover the theory behind the techniques, but it is the hope that this workshop will whet your appetite.\n",
    "\n",
    "\n",
    "The reader is directed to these resources as a starting point for further learning.\n",
    "\n",
    "### Software:\n",
    "- Scikit-Learn <a href=\"http://scikit-learn.org/stable/documentation.html\">documentation</a>, <a href=\"http://scikit-learn.org/stable/auto_examples/index.html\">example gallery</a>\n",
    "- Complementary packages: <a href=\"http://statsmodels.sourceforge.net/\">statsmodel</a>, <a href=\"http://dan.iel.fm/emcee/current/\">emcee</a>, <a href=\"http://www.nltk.org/\">NLTK</a>, <a href=\"http://deeplearning.net/software/theano/\">Theano</a>, <a href=\"https://github.com/lisa-lab/pylearn2\">Pylearn2</a>, <a href=\"https://github.com/JasperSnoek/spearmint\">spearmint</a>, ...\n",
    "- <a href=\"https://www.tensorflow.org/\">TensorFlow</a> for highly efficient numerical operations, typically used for deep neural networks, on both GPUs and CPUs, on single machines and distributed over many devices\n",
    "\n",
    "### Datasets:\n",
    "- <a href=\"https://archive.ics.uci.edu/ml/index.php\">The UCI Machine Learning Repo has 417 datasets that are freely available for you to play with. </a>\n",
    "- <a href='http://kaggle.com'>Kaggle is a social network</a> for data science, it hosts competitions and data sets\n",
    "\n",
    "### Online Courses:\n",
    "- Machine Learning course taught by Andrew Ng (also available on YouTube) https://www.coursera.org/learn/machine-learning there are a ton of outher courses on coursera, edx, and youtube.\n",
    "- (An alternative course would be this https://work.caltech.edu/telecourse.html) \n",
    "\n",
    "### Books\n",
    "- <a href=\"http://www.springer.com/gb/book/9780387310732\">Pattern Recognition and Machine Learning, Bishop</a>\n",
    "- <a href=\"http://www-bcf.usc.edu/~gareth/ISL/\"> An Introduction to Statistical Learning, Tibshirani et al.</a>\n",
    "- <a href=\"http://neuralnetworksanddeeplearning.com/\">Neural Networks and Deep Learning, Neilson</a>\n",
    "- <a href=\"http://www.deeplearningbook.org/\">Deep Learning, Goodfellow et al.</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is machine learning?\n",
    "\n",
    "If you already have a high level, conceptual understanding of ML, feel free to skip this section!\n",
    "\n",
    "Machine Learning aims to <strong> predict, understand or identify patterns in data </strong> from observations.\n",
    "\n",
    "<img src=\"img/ml_diag.png\"></img>\n",
    "\n",
    "## Supervised Learning\n",
    "<strong>Supervised learning</strong> - data comes with additional attributes that we want to predict\n",
    "\n",
    "Supervised Learning is generally split into <strong> classification </strong> and <strong>regression</strong>.\n",
    "\n",
    "### Classification\n",
    "\n",
    "In <strong>classification</strong>, samples belong to two or more <strong>classes</strong> and we want to learn from already labeled data how to predict the class of unlabelled data.\n",
    "<br>\n",
    "For example, imagine we have a set of photos of dogs, and the corresponding breed for each dog.\n",
    "<img src=\"img/class_doggos.png\"></img>\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, imagine you have a photo of a dog, but do not know what breed it is.\n",
    "<img src=\"img/unseen_doggo.png\"></img>\n",
    "The goal of learning is then to train a model which, when presented with an unseen photo of a dog, can correctly label it as \"Labrador Retriever\"\n",
    "<br>\n",
    "That is, we want to find a function $f$, such that:\n",
    "<img src=\"img/doggo_equation.png\"></img>\n",
    "\n",
    "<img src=\"img/label_unseen.png\"></img>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "### Regression\n",
    "In <strong>regression</strong>, you try to predict one or more continuous variables. \n",
    "\n",
    "<br>\n",
    "For example, imagine we have a set of heights of dogs, and respective their weights.\n",
    "<img src=\"img/regress.png\"></img>\n",
    "The goal of learning is to train a model which, given the height of a new dog, can correctly predict the dog's weight.\n",
    "<img src=\"img/regress_unkown.png\"></img>\n",
    "<br>\n",
    "<img src=\"img/dog_regress_eqn.png\"></img>\n",
    "\n",
    "\n",
    "## Unsupervised Learning\n",
    "In <strong>unsupervised learning</strong>, data consists of a set of input vectors x without any corresponding target values. Unsupervised learning is <strong>not</strong> covered by this workshop, for more information <a href=\"https://en.wikipedia.org/wiki/Unsupervised_learning\">look here.</a>\n",
    "\n",
    "<br>\n",
    "For example, imagine you have a set of photos of dogs, but you aren't trying to predict a corresponding label or attribute. Instead, you are concerned with the relationships between and structure of the data.\n",
    "\n",
    "<img src=\"img/unsuper_dogs.png\"></img>\n",
    "<br>\n",
    "<br>\n",
    "Unsupervised Learning can take the form of <strong>clustering</strong>, which aims to discover groups of similar examples within the data.\n",
    "\n",
    "For example, given a set of dogs, try to group them into groups of similar dogs. \n",
    "<img src=\"img/cluster_dogs.png\"></img>\n",
    "\n",
    "Some other types of unsupervised learning are:\n",
    "<ul>\n",
    "<li><strong>Anomaly detection </strong> </li>\n",
    "<li><strong>Autoencoders</strong> </li>\n",
    "<li><a href=\"https://en.wikipedia.org/wiki/Unsupervised_learning#Approaches\">Many more</a></li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "<strong>Applications include:</strong> _Natural language processing, Computer vision, IR and advertisement, Robotics, Bioinformatics, High Energy Physics, ..._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will be using Scikit-learn (sklearn) throughout this workshop. Here's a brief overview of what it is:\n",
    "## Overview\n",
    "\n",
    "* Machine learning library written in __Python__\n",
    "* __Simple and efficient__, for both experts and non-experts\n",
    "* Classical, __well-established machine learning algorithms__\n",
    "* Shipped with <a href=\"http://scikit-learn.org/dev/documentation.html\">documentation</a> and <a href=\"http://scikit-learn.org/dev/auto_examples/index.html\">examples</a>\n",
    "* __BSD 3 license__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Python stack for data analysis\n",
    "\n",
    "- The __open source__ Python ecosystem provides __a standalone, versatile and powerful scientific working environment__, including: [NumPy](http://numpy.org), [SciPy](http://scipy.org), [IPython](http://ipython.org), [Matplotlib](http://matplotlib.org), [Pandas](http://pandas.pydata.org/), _and many others..._\n",
    "\n",
    "<center> \n",
    "<img src=\"img/scikit-learn-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"img/numpy-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"img/scipy-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"img/ipython-logo.jpg\" style=\"max-width: 120px; display: inline\" />\n",
    "<img src=\"img/matplotlib-logo.png\" style=\"max-width: 120px; display: inline\"/>\n",
    "<img src=\"img/pandas-logo.png\" style=\"max-width: 120px; display: inline\" />\n",
    "</center>\n",
    "\n",
    "- Scikit-Learn builds upon NumPy and SciPy and __complements__ this scientific environment with machine learning algorithms;\n",
    "- By design, Scikit-Learn is __non-intrusive__, easy to use and easy to combine with other libraries;\n",
    "- Core algorithms are implemented in low-level languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Algorithms in SKLearn\n",
    "\n",
    "__Supervised learning:__\n",
    "\n",
    "* Linear models (Ridge, Lasso, Elastic Net, ...)\n",
    "* Ensemble methods (Random Forests, Bagging, GBRT, ...)\n",
    "* Support Vector Machines\n",
    "* Nearest neighbors\n",
    "* Neural networks \n",
    "\n",
    "<center><a href=\"http://scikit-learn.org/dev/auto_examples/classification/plot_classifier_comparison.html\"><img src=\"img/classifiers.png\" width=\"90%\" /></a>\n",
    "<em>A comparison of (some of the) classifiers in Scikit-Learn</em></center><br />\n",
    "\n",
    "__Unsupervised learning:__\n",
    "\n",
    "* Clustering (KMeans, Ward, ...)\n",
    "* Matrix decomposition (PCA, ICA, ...)\n",
    "* Density estimation\n",
    "* Outlier detection\n",
    "\n",
    "__Model selection and evaluation:__\n",
    "\n",
    "* Cross-validation\n",
    "* Grid-search\n",
    "* Lots of metrics\n",
    "\n",
    "_... and many more!_ (See the [Reference](http://scikit-learn.org/stable/modules/classes.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A Practical Example: Classifying digits \n",
    "\n",
    "Let's work through an example of using a machine learning classifier to label images of handwritten digits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>Note:</strong> If you do not have a linear algebra background, the term \"dimensional\" may sound intimidating. It shouldn't be -- here's some informal intuition behind the concept:\n",
    "\n",
    "We perceive 3 dimensions in our everyday lives, and we can visualise what 2D is, and what 3D is pretty easily.\n",
    "\n",
    "<img src=\"img/2d3d.png\"></img>\n",
    "\n",
    "A point $p$ in 2D space can be represented by two coordinates, or two *dimensions*, such as in the image above $\\textbf{p}=[x,y]$.\n",
    "\n",
    "A point $p$ in 3D space can be represented by three coordinates, or three *dimensions*, such as in the image above $\\textbf{p}=[x,y,z]$.\n",
    "\n",
    "When one says $n$ dimensions, they are describing a space with $n$ degrees a freedom, where a point (or direction) can be represented by a vector with $n$ elements. While we cannot visualise it, a point $p$ in ND space can be represented by a vector of $n$ coordinates $\\textbf{p}=[p_1,p_2,...,p_n]$\n",
    "\n",
    "\n",
    "Now, let's get back to the digits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Input data\n",
    "\n",
    "The <strong>digits dataset</strong> consists of 1797 images of handwritten digits. \n",
    "\n",
    "Each image of a digit, referred to as a sample, is represented by a 64 dimensional vector of integers $x \\in \\mathbb{Z}^{64}$. The elements of this vector are often referred to as the `features` of the sample.\n",
    "\n",
    "Each image of a digit has a corresponding digit label, $t \\in \\{0,1,2,3,4,5,6,7,8,9\\}$. This label is often referred to as the `class` or `target`.\n",
    "\n",
    "\n",
    "We want to *classify* the images, i.e. predicting what digit is in the image.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the dataset.\n",
    "import sklearn.datasets\n",
    "digits = sklearn.datasets.load_digits(n_class=10)\n",
    "print(\"The dataset has {} sample digits, each represented as a {} dimensional feature vector.\".format(digits.data.shape[0], digits.data.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick one of the samples from the dataset at random and have a look at it.\n",
    "\n",
    "The cell below prints the feature vector of the randomly selected image. \n",
    "\n",
    "<strong>Note:</strong> Images on computers are stored as arrays of numbers. The value of each number tells the computer what colour the 'pixel' of the image should be. In our case, each image of a digit has 64 'pixels'. The integer value of each element of the feature vector $x$ indicates how dark or light the corresponding pixel should be. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import randint\n",
    "# Pick a digit at random.  \n",
    "j = randint(digits.data.shape[0])\n",
    "\n",
    "# Print the feature vector of the randomly selected digit.\n",
    "print(\"Feature vector of random image is x=\", digits.data[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot these values by reshaping the $64$ dimensional vector into an $8x8$ matrix. In this plot, the square at the position $n,m$ is white if the $n,m$th element of the matrix is equal to 0, and is increasingly darker as the value rises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the data image, reshaping the 64 dimensional vector as an 8x8 matrix.\n",
    "imshow(reshape(digits.data[j], (8,8)), cmap=cm.gray_r)\n",
    "title(\"Digit: %d\" % digits.target[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good! Now that we know what our data actually looks like, let's introduce more *jargon*.\n",
    "\n",
    "In machine learning, you *train* your model by showing it known feature-target pairs of data, called the <strong>training set</strong>, and evaluate how well the model is able to predict targets given features on a *separate* dataset called the <strong>test set</strong>. You already know what the true label is for the test set, so you can compare the predictions to the true values and this gives you an idea of how well the model would work in the real world.\n",
    "\n",
    "\n",
    "<img src=\"img/train_test.png\"></img>\n",
    "\n",
    "\n",
    "\n",
    "Ideally, you only evaluate on the <strong>test set</strong> *after all tweaking and training* of your models has been completed. You want the performance on the test set to be an unbiased representation of how the model would perform on data in the real world. However, it is useful to have an intermediate idea of how well the model is doing, this is accomplished with a <strong>validation set</strong>.\n",
    "\n",
    "In our example, we are going to split the digits dataset into three equal sets to create a <strong>training set</strong>, <strong>a validation set </strong> and <strong>a test set</strong>.\n",
    "\n",
    "The digits in the <strong>training set</strong> will be used to train the model. The digits in the <strong>validation set</strong> will be used to evaluate how well our model is performing. When we are <strong>completely finished</strong> training we can test how well the model works by using it to classify the digits in the <strong>test set</strong>.\n",
    "\n",
    "\n",
    "\n",
    "Let's randomly split our dataset into three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indeces = digits.data.shape[0]\n",
    "# Shuffle the indeces of the digits dataset.\n",
    "shuffled_indeces = permutation(indeces)\n",
    "\n",
    "# Split the shuffled indeces into three equal parts.\n",
    "training_indeces = shuffled_indeces[:indeces//3]\n",
    "validation_indeces = shuffled_indeces[indeces//3:2*(indeces//3)]\n",
    "test_indeces = shuffled_indeces[2*(indeces//3):]\n",
    "\n",
    "# Select the digits indexed by training_indeces to be the training set\n",
    "# Xtr contains the training features\n",
    "Xtr = digits.data[training_indeces]\n",
    "# ttr contains the corresponding targets (class label) for each\n",
    "# digit in the training set\n",
    "ttr = digits.target[training_indeces]\n",
    "\n",
    "# Same for the validation set.\n",
    "Xv = digits.data[validation_indeces]\n",
    "tv = digits.target[validation_indeces]\n",
    "\n",
    "# These will *only* be used for evaluation, when training is completed.\n",
    "# Select the digits indexed by test_indeces to be the test set\n",
    "# Xte contains the test features\n",
    "Xte = digits.data[test_indeces]\n",
    "# tte contains the corresponding test targets\n",
    "tte = digits.target[test_indeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"There are {} digits in the training set\".format(Xtr.shape[0]))\n",
    "print(\"There are {} digits in the validation set\".format(Xv.shape[0]))\n",
    "print(\"There are {} digits in the test set\".format(Xte.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data in Scikit-Learn\n",
    "\n",
    "- Input data = Numpy arrays or sparse matrices ;\n",
    "- Leverage efficient vector operations ;\n",
    "- Keep code short and readable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## K-Nearest Neighbor \n",
    "\n",
    "To start, we will use the <a href=\"https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\">$k$-nearest neighbors classifier</a> to classify our digits. \n",
    "\n",
    "\n",
    "\n",
    "The $k$-nearest neighbor classifier predicts the class of an unknown sample $x$ as the most common class among the $k$ 'nearest' points in the training set, where 'nearest' is defined by some distance metric, such as <a href='https://en.wikipedia.org/wiki/Euclidean_distance'>euclidean distance</a>.\n",
    "\n",
    "e.g. \n",
    "The euclidean distance $d(*,*)$ between two points $a$ and $b$ where $a,b \\in \\mathbb{R^n}$\n",
    "$$d(a, b)  = \\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2+\\cdots+(a_i - b_i)^2+\\cdots+(a_n - b_n)^2}$$\n",
    "\n",
    "\n",
    "You can find all the <a href='http://scikit-learn.org/stable/modules/classes.html#pairwise-metrics'>distance metrics available in sklearn here</a>.\n",
    "\n",
    "\n",
    "<img src=\"img/knn.png\"></img>\n",
    "\n",
    "In the diagram above, we have a training set consisting of all the yellow and purple circles. For each sample in our training set we have a feature vector $\\textbf{x}= [x_1,x_2]$ and a corresponding label, $t \\in \\{A,B\\}$.\n",
    "\n",
    "Now, imagine we want to classify a new sample, the red star.  We know the feature vector of the red star $[x_1,x_2]$, but we do not know which class it is in. \n",
    "\n",
    "To classify the red star with the K Nearest Neighbors Classifier, we calculate what is the most commonly occurring class in the k closest neighboring samples. If $k=3$, then we predict that the red star is in class B. If $k=6$, then we predict the red star is in class A.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances \n",
    "# Let's calculate the Euclidean distance between the 5th and 6th\n",
    "# entries in the training set.\n",
    "d = euclidean_distances(Xtr[5:7])\n",
    "# d is a numpy array, where the (i,j)th entry indicates the distance \n",
    "# between the ith and jth samples.`\n",
    "print(\"The pairwise euclidean distance between the 5th and 6th samples is:\")\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Can you calculate the manhattan distance between the 5th and 6th sample?\n",
    "# hint -- check the distance metrics documentation link above.\n",
    "manhattan_d = \"\"\"evaluate the manhattan distance\"\"\" \n",
    "print(\"The pairwise manhattan distance between the 5th and 6th samples is:\")\n",
    "print(manhattan_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default distance metric used in SKLearn implementation of the KNeighborsClassifier is the <a href=\"https://en.wikipedia.org/wiki/Minkowski_distance\">minkowski distance</a>, a generalisation of the euclidean and manhattan distances.\n",
    "\n",
    "$d(A,B)=\\left(\\sum_{i=1}^n |a_i-b_i|^p\\right)^{1/p}$\n",
    "\n",
    "(When p=2, the default value for sklearn, this is the euclidean distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A simple and unified API\n",
    "\n",
    "All learning algorithms in scikit-learn share a uniform and limited API consisting of complementary interfaces:\n",
    "\n",
    "- an `estimator` interface for building and fitting models;\n",
    "- a `predictor` interface for making predictions;\n",
    "- (a `transformer` interface for converting data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimators\n",
    "\n",
    "An estimator is any object that learns from data; it may be a classification, regression or clustering algorithm or a transformer that extracts/filters useful features from raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Estimator(object):\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fits estimator to data.\"\"\"\n",
    "        # set state of ``self``\n",
    "        return self\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict response of ``X``.\"\"\"\n",
    "        # compute predictions ``pred``\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the k-nearest neighbor classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Create a K-nearest neighbor classifier, where the 2 nearest neighbors \n",
    "# are considered.\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "# Fit the model to the training data by calling the fit method of knn,\n",
    "# and passing the training features Xtr and targets ttr.\n",
    "knn.\"\"\"CALL THE FIT METHOD\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Congrats, you've just made your first machine learning model!\n",
    "\n",
    "### Predictors\n",
    "Now let's see how well it performs on the <strong>validation set</strong>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Predict the classes, y, of the images in the validation set, Xv.\n",
    "# by calling the predict method of knn.\n",
    "y = knn.\"\"\"CALL THE PREDICT METHOD\"\"\"\n",
    "\n",
    "# Calculate the validation accuracy\n",
    "# i.e. For what percent of the digits in Xv did our model \n",
    "# correctly predict the digit label tv\n",
    "val_acc = accuracy_score(tv, y)\n",
    "print(\"The KNN Classifier with k=2 has a validation accuracy of \", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model evaluation and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Cross Validation\n",
    "Instead of splitting our dataset into 1/3 training and 1/3 validation, we can combine these sets to form one training set, and use <a href=\"http://scikit-learn.org/stable/modules/cross_validation.html\">k-fold cross validation</a> to validate the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"img/kfold.jpg\" width=\"70%\" /></center>\n",
    "\n",
    "In KFold Cross validation, the training set is split into $k$ subsets.  $k-1$ subsets are used to train the model, and the remaining $1$ subset is used to evaluate its performance. \n",
    "\n",
    "This allows you to use a larger portion of the available data to train the model, while still validating its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# Let's recombine our training and validation set now that we are \n",
    "# using KFold Cross Validation.\n",
    "Xtr = np.concatenate((Xtr, Xv), axis=0)\n",
    "ttr = np.concatenate((ttr, tv), axis=0)\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for train, valid in KFold(n=len(Xtr), n_folds=5, random_state=42):\n",
    "    X_tr, t_tr = Xtr[train], ttr[train]\n",
    "    X_v, y_v = Xtr[valid], ttr[valid]\n",
    "    knn = KNeighborsClassifier().fit(X_tr, t_tr)\n",
    "    scores.append(knn.score(X_v, y_v))\n",
    "\n",
    "print(\"Mean Validation accuracy using K-folds cross validation %f +-%f:\" % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shortcut\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(KNeighborsClassifier(), \n",
    "                         Xtr, ttr, cv=KFold(n=len(X), n_folds=5, random_state=42), \n",
    "                         scoring=\"accuracy\")\n",
    "print(\"%f +-%f\" % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Under- and over-fitting\n",
    "\n",
    "- <strong>Under-fitting:</strong> the model is too simple and does not capture the true relation between X (the image vector) and t (the digit label).\n",
    "- <strong>Over-fitting:</strong> the model is too specific to the training set and does not generalisation to new, previously unseen images of digits.\n",
    "\n",
    "\n",
    "Let's see how the model performs when fit with a range of different values of k.\n",
    "\n",
    "When is it overfitting? When is it underfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.learning_curve import validation_curve\n",
    "\n",
    "# Let's see how well the model performs for each value of k from 1 too 100\n",
    "param_range = range(1, 100)\n",
    "train_scores, valid_scores = validation_curve(\n",
    "    KNeighborsClassifier(), Xtr, ttr, \n",
    "    param_name=\"n_neighbors\", \n",
    "    param_range=param_range, cv=5, n_jobs=-1)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "valid_scores_mean = np.mean(valid_scores, axis=1)\n",
    "valid_scores_std = np.std(valid_scores, axis=1)\n",
    "\n",
    "plt.xlabel(\"k (n_neighbors)\")\n",
    "plt.ylabel(\"score\")\n",
    "plt.xlim(min(param_range), max(param_range))\n",
    "plt.plot(param_range, train_scores_mean, color=\"red\", label=\"training score\")\n",
    "plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.2, color=\"red\")\n",
    "plt.plot(param_range, valid_scores_mean, color=\"blue\", label=\"validation score\")\n",
    "plt.fill_between(param_range, valid_scores_mean - valid_scores_std,\n",
    "                 valid_scores_mean + valid_scores_std, alpha=0.2, color=\"blue\")\n",
    "plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Best trade-off\n",
    "print(\"max_depth = %d, accuracy = %f\" % (param_range[np.argmax(valid_scores_mean)],\n",
    "                                         np.max(valid_scores_mean)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Hyper-parameter search\n",
    "\n",
    "- Learning algorithms are not black boxes, their execution is controlled by Hyper-parameters, such as the value of the number of neighbors, k, to consider. \n",
    "- Finding good hyper-parameters is crucial to control under- and over-fitting, hence achieving better performance.\n",
    "- We can automatically fit and evaluate the classifier with each combination of a selection of hyperparameters with the `GridSearchCV` estimator.\n",
    "\n",
    "You can see a complete list of the parameters of the Sklearn KNN Classifier implementation <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\">in the documentation here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Evaluate the performance of the classifier with each value of k from\n",
    "# 1 to 100, using the manhattan and euclidean distance metrics, using\n",
    "# different strategies for weighting the distances\n",
    "grid = GridSearchCV(KNeighborsClassifier(),\n",
    "                    param_grid={\"n_neighbors\": [\"\"\"FILL THIS WITH RANGE OF K\"\"\"],\n",
    "                                \"metric\": [ \"\"\"FILL THIS IN WITH METRICS\"\"\"],\n",
    "                                \"weights\" : [\"\"\"FILL THIS IN WITH WEIGHTS\"\"\"]\n",
    "                                },\n",
    "                    scoring=\"accuracy\",\n",
    "                    cv=5, n_jobs=-1)\n",
    "\n",
    "grid.\"\"\"FIT THE GRID BY CALLING THE FIT METHOD\"\"\"\n",
    "\n",
    "print(\"Best score = %f, Best parameters = %s\" % (grid.best_score_, \n",
    "                                                 grid.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Going further\n",
    "\n",
    "Now that we've successfully fit the KNeighborsClassifier, let's try some others. Have a look at the <a href=\"http://scikit-learn.org/stable/supervised_learning.html#supervised-learning\"> SKLearn Documentation to see what is available. </a>\n",
    "\n",
    "Not sure what to try? Why not the <a href=\"http://scikit-learn.org/stable/modules/svm.html#multi-class-classification\">Multi-class SVM implementation, `SVC` </a>\n",
    "\n",
    "\n",
    "Try to do a `GridSearchCV` search, like we did with the KNeighborsClassifier, to select good hyperparameters for the model.\n",
    "\n",
    "<strong> Note: If you want to add more cells here, go to Menu > Insert > Cell Below </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## When you're all done\n",
    "When you are done tinkering with different classifiers, test the accuracy of your classifier's predictions on the test set, Xte and tte.\n",
    "\n",
    "Use the predict method of the classifier, and accuracy_score to compare the predictions to the actual labels in tte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How does your classifier perform?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Model persistence\n",
    "\n",
    "As any Python objects, estimators can be saved to disk for future reuse using `pickle`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_class = KNeighborsClassifier().fit(Xtr, ttr)\n",
    "\n",
    "# Save to disk\n",
    "import pickle\n",
    "pickle.dump(knn_class, open(\"my-knn.dat\", \"wb\"))\n",
    "\n",
    "# Load from disk\n",
    "pickle.load(open(\"my-knn.dat\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Advanced topics\n",
    "\n",
    "Scikit-Learn is more than training classifiers. It also covers:\n",
    "\n",
    "- Clustering\n",
    "- Matrix decomposition\n",
    "- Kernel Density Estimation\n",
    "- Outlier detection\n",
    "- Out-of-core learning\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Further readings\n",
    "\n",
    "### Software:\n",
    "- Scikit-Learn <a href=\"http://scikit-learn.org/stable/documentation.html\">documentation</a>, <a href=\"http://scikit-learn.org/stable/auto_examples/index.html\">example gallery</a>\n",
    "- Complementary packages: <a href=\"http://statsmodels.sourceforge.net/\">statsmodel</a>, <a href=\"http://dan.iel.fm/emcee/current/\">emcee</a>, <a href=\"http://www.nltk.org/\">NLTK</a>, <a href=\"http://deeplearning.net/software/theano/\">Theano</a>, <a href=\"https://github.com/lisa-lab/pylearn2\">Pylearn2</a>, <a href=\"https://github.com/JasperSnoek/spearmint\">spearmint</a>, ...\n",
    "- <a href=\"https://www.tensorflow.org/\">TensorFlow</a> for highly efficient numerical operations, typically used for deep neural networks, on both GPUs and CPUs, on single machines and distributed over many devices<img src='img/tensorflow.png'></img>\n",
    "\n",
    "### Datasets:\n",
    "- <a href=\"https://archive.ics.uci.edu/ml/index.php\">The UCI Machine Learning Repo has 417 datasets that are freely available for you to play with. </a>\n",
    "- <a href='http://kaggle.com'>Kaggle is a social network</a> for data science, it hosts competitions and data sets\n",
    "\n",
    "### Online Courses:\n",
    "- Machine Learning course taught by Andrew Ng (also available on YouTube) https://www.coursera.org/learn/machine-learning there are a ton of outher courses on coursera, edx, and youtube.\n",
    "- (An alternative course would be this https://work.caltech.edu/telecourse.html) \n",
    "\n",
    "### Books\n",
    "- <a href=\"http://www.springer.com/gb/book/9780387310732\">Pattern Recognition and Machine Learning, Bishop</a>\n",
    "- <a href=\"http://www-bcf.usc.edu/~gareth/ISL/\"> An Introduction to Statistical Learning, Tibshirani et al.</a>\n",
    "- <a href=\"http://neuralnetworksanddeeplearning.com/\">Neural Networks and Deep Learning, Neilson</a>\n",
    "- <a href=\"http://www.deeplearningbook.org/\">Deep Learning, Goodfellow et al.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Scikit-Learn provides all essential tools for machine learning.\n",
    "- It integrates within a larger Python scientific ecosystem.\n",
    "- Try it for yourself!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
